% acmlarge-sample.tex, dated 25th May 2012
% This is a sample file for ACM large trim journals
%
% Compilation using 'acmlarge.cls' - version 1.3, Aptara Inc.
% (c) 2011 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - May 2012

\documentclass[prodmode,acmtap]{acmlarge}

% Metadata Information
\acmVolume{2}
\acmNumber{3}
\acmArticle{1}
\articleSeq{1}
\acmYear{2010}
\acmMonth{5}

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\SetAlFnt{\algofont}
\SetAlCapFnt{\algofont}
\SetAlCapNameFnt{\algofont}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
\renewcommand{\algorithmcfname}{ALGORITHM}


% Page heads
\markboth{Daniel Watzinger}{Neural Modeling of Flow Rendering Effectiveness}

% Title portion
\title{Neural Modeling of Flow Rendering Effectiveness}
\author{Daniel Watzinger \affil{University of Passau}}

\begin{abstract}
It has been previously proposed that understanding the mechanisms of
contour perception can provide a theory for why some flow-rendering
methods allow for better judgments of advection pathways than others.
In the present article, we develop this theory through a numerical
model of the primary visual cortex of the brain (Visual Area 1) where
contour enhancement is understood to occur according to most
neurological theories. We apply a two-stage model of contour
perception to various visual representations of flow fields evaluated
using the advection task of Laidlaw et al. [2001]. In the first
stage, contour {enhancement} is modeled based on Li's cortical model
[Li 1998]. In the second stage, a model of streamline {tracing} is
proposed, designed to support the advection task. We examine the
predictive power of the model by comparing its performance to that of
human subjects on the advection task with four different
visualizations. The results show the same overall pattern for humans
and the model. In both cases, the best performance was obtained with
an aligned streamline-based method, which tied with a LIC-based
method. Using a regular or jittered grid of arrows produced worse
results. The model yields insights into the relative strengths of
different flow visualization methods for the task of visualizing
advection pathways.
\end{abstract}

\category{H.5.2}{Information Interfaces and Presentation}{User
Interfaces}[Evaluation/\break methodology]
\category{H.1.2}{Models and Principles}{User/Machine Systems}[Human Information Processing]
\category{I.5.1}{Pattern\break Recognition}{Models}[Neural Nets]

\terms{Human Factors}
\keywords{Contour perception, flow visualization, perceptual theory, visual cortex, visualization}

\acmformat{Pineo, D. and Ware,  C. 2010.Neural Modeling of Flow Rendering Effectiveness.}

\begin{document}

\begin{bottomstuff}
This work is supported by the Widget Corporation Grant \#312-001.\\
Author's address: D. Pineo, Kingsbury Hall, 33 Academic Way, Durham,
N.H. 03824; email: dspineo@comcast.net; Colin Ware, Jere A. Chase
Ocean Engineering Lab, 24 Colovos Road, Durham, NH 03824; email: cware@ccom.unh.edu
\end{bottomstuff}
\maketitle

% Head 1
\section{Introduction}

Many techniques for 2D flow visualization have been developed and
applied. These include grids of little arrows, still the most common
for many applications, equally spaced streamlines
\cite{Turk1996,Jobard1997}, and line integral convolution (LIC)
\cite{Cabral1993}. But which is best and why? \citeN{Laidlaw2001}
showed that the ``which is best'' question can be answered by means
of user studies in which participants are asked to carry out tasks
such as tracing advection pathways or finding critical points in the
flow field. (Note: An advection pathway is the same as a streamline
in a steady flow field.) \citeN{Ware2008} proposed that the ``why''
question may be answered through the application of recent theories
of the way contours in the environment are processed in the visual
cortex of the brain. But Ware only provided a descriptive sketch
with minimal detail and no formal expression. In the present paper,
we show, through a numerical model of neural processing in the
cortex, how the theory predicts which methods will be best for an
advection path tracing task.

% Head 2
\subsection{The IBQ Approach in Image Quality Estimation}

The IBQ approach combined with psychometric methods has proven suitable,
especially for testing the performance of imaging devices or their
components and then returning this quality information to the product
development or evaluation stages. When the subjective changes in image
quality are multivariate, the technical parameters changing in the test
image are unknown or difficult to compute. However, the IBQ approach can be
used to determine the subjectively important quality dimensions with a wide
range of natural image material related to changes caused by different
devices or their components. In order to tune the image-processing
components for optimal performance, it is important to know what the
subjectively crucial characteristics that change in the perceived image
quality are as a function of the tuning parameters, or simply for different
components. Table I describes the problems caused by multivariate changes in
image quality and offers suggestions of how to approach them by using
different measurement methods that complement each other. The IBQ approach
can complement the psychometric approaches and objective measurements by
defining the subjective meaning of image quality attributes and
characteristics; in other words, it reveals how important they are for the
overall perceived quality. This information can then be used as guidance in
tuning, and no complex models are needed in order to understand the relation
between objective measures and subjective quality ratings.

\section{Discussion}
The overall agreement between the pattern of results for human
observers and the V1-based model provides strong support of the
perceptual theory we outlined in the introduction. The aligned arrows
style of visualization produced clear chains of mutually reinforcing
neurons along the flow path in the representation, making the flow
pathway easy to trace as predicted by theory.

The fact that LIC produced results as good as the equally spaced
streamlines was something of a surprise, and this lends support to
its popularity within the visualization community. While it did not
produce as much neuron excitation as the aligned arrows method, this
was offset by the lack of nontangential edge responses produced by
glyph-based visualizations. However, its good performance was
achieved only because our evaluation method ignored the directional
ambiguity inherent in this method. \citeN{Laidlaw2001} found this
method to be the worst and there is little doubt that had we allowed
flow in any direction, up or down, human observers would have found
pathways with close to 180 degrees of error half of the time.

The performance of both the model and the human test subjects is
likely to be highly dependent on the underlying vector field used.
As described in Section 5.1.6, the vector field was generated by
interpolating between an 8x8 grid of random, but generally upward
pointing vectors. A consequence of this is that when adjacent vectors
in this grid point somewhat toward each other, the vector field forms
an area of convergence. This convergence area tends to funnel
neighboring streamline paths together, reducing error in streamline
tracing (Figure \ref{regularfig} is an example of this).  Thus, the
overall accuracies of both the model and human subjects may be higher
than might be might be observed using a vector field without such convergence zones.

We were surprised that the computer algorithm actually did better at
the task than human observers. One reason for this may have been that
humans would have to make saccadic eye movements to trace a path,
whereas the computer did not. For the patterns we used, it is likely
that the observers had to make fixations on several successive parts
of a path, and errors may have accumulated as they resumed a trace
from a previous fixation. Nevertheless, we feel that the algorithm
could easily be adjusted to make it give results closer to human
subjects. A more sophisticated approach would be to simulate eye fixations.

The model we applied is a considerable simplification over what
actually occurs. It only uses the simplest model of the simplest
orientation sensitive neurons, and fails to include cortical
magnification, among other shortcomings. Real cortical receptive
fields are not arranged in a rigid hexagonal grid as they are in Li's
model. Furthermore, the neurons of V1 respond to many frequencies,
however our model only uses one in its present form. In addition,
besides the so-called simple cells modeled by \citeN{Li1998a}, other
neurons in V1 and V2 called complex and hypercomplex cells all have
important functions. For example, end-stopped cell respond best to a
contour that terminates in the receptive field and understanding
these may be important in showing how the direction of flow along a
contour can be unambiguously shown. Moreover, visual information is
processed through several stages following the primary cortex,
including V2, V4 and the IT cortex. Each of these appears to abstract
more complex, less localized patterns. Researchers are far from
having sufficient information to model the operations of these stages
all of which may have a role in tracing contours. Nevertheless, the
results are compelling and there are advantages in having a
relatively simple model. We have plans to add some of these more
complex functions in future versions of the model.



% Appendix
\appendix
\section*{APPENDIX}
\setcounter{section}{1}


% Bibliography
\bibliographystyle{acmlarge}
\bibliography{acmlarge-sam}

% History dates
\received{February 2009}{July 2009}{October 2009}


\elecappendix


\section{Analysis of Invalid Trials}
\label{invalid}

\subsection{Results}


Invalid trials were previously defined as those trials in which the
subject pressed the space bar to end the trial without first bringing
the virtual finger to a stop. The number of invalid trials for each
subject is presented by feedback condition in Figure~12. Due to the
irregular distribution of the data, no significance test was run.
However, the figure shows two notable features. First, Subject 6 had
more invalid trials than any other subject. Second, more invalid
trials occurred under the proprioceptive-only (NV$+$P) feedback
condition than any other.



\subsection{Discussion}

Although the number of invalid trials is not directly related to task
performance, we now consider any trends that may be seen in this
information. No statistical tests were done with this data, but some
inferences can be drawn from the invalid trial counts in Figure 12.
The only obvious trend is that the NV$+$P condition appears to have
the most invalid trials, which is the case for all but two subjects.
In the post-experiment survey, one subject commented on this trend,
saying that with only proprioceptive motion feedback it was hard to
tell if the finger was moving or not. This might be a result of a
larger threshold for absolute motion detection for proprioceptive
feedback than for visual feedback. This difficulty in stopping the
finger did not appear to affect the ease of use ratings provided by
subjects, as no correlation was observed with invalid trial counts.

It is interesting to note that the no-feedback condition (NV$+$NP)
had fewer invalid trials than the proprioceptive-only condition
(NV$+$P), especially in light of the findings of Ghez et al. [1990]
that deafferented individuals tend to display endpoint drift in
non-sighted targeted reaching movements (equivalent to NV$+$NP
condition) while neurologically normal individuals do not (equivalent
to NV$+$P condition). A notable difference between our study and the
study by Ghez et al.\ is the availability of kinesthetic feedback
from the thumb pressing on the force sensor, which indicates the
magnitude of the applied force, that is, the movement command in our
study. Thus, under the no-feedback condition, subjects could use this
information to learn to apply grasping forces within the dead zone to
stop finger movement. When motion feedback is available, subjects are
likely focusing more on the feedback than on the forces applied,
since the feedback allows them to achieve better accuracy. Thus, at
the end of a trial, subjects are most likely using this feedback as
an indicator of zero velocity rather than attending to the applied
force. When visual feedback is available, it is easy to determine
whether the finger is moving or not; however, when only
proprioceptive feedback is available, the finger can be moving slowly
without the subject being aware of its motion. This explanation would
result in a larger number of failed trials for the NV$+$P condition
than for any other, as observed.

\end{document}

